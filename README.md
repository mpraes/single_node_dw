# Single Node DW

Framework de Data Warehouse single-node para pipelines ETL com conectores de origem e destino.

## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Arquitetura](#arquitetura)
- [Caracter√≠sticas Principais](#caracter√≠sticas-principais)
- [Conectores Dispon√≠veis](#conectores-dispon√≠veis)
- [Instala√ß√£o](#instala√ß√£o)
- [Configura√ß√£o](#configura√ß√£o)
- [Uso](#uso)
- [Exemplos Pr√°ticos](#exemplos-pr√°ticos)
- [Integra√ß√£o com Mage.ai](#integra√ß√£o-com-mageai)
- [Testes](#testes)
- [Infraestrutura](#infraestrutura)
- [Documenta√ß√£o](#documenta√ß√£o)
- [Estrutura do Projeto](#estrutura-do-projeto)

## üéØ Vis√£o Geral

O **Single Node DW** √© um framework Python moderno projetado para cen√°rios de ETL que requerem **baixo custo operacional** e **alta efici√™ncia** em ambientes de dados pequenos e m√©dios. 

A proposta √© reduzir a complexidade de infraestrutura sem comprometer a organiza√ß√£o, rastreabilidade e qualidade do processo de ETL.

### Objetivos

- **Consolida√ß√£o de dados**: Integrar m√∫ltiplas fontes em um ambiente √∫nico e estruturado
- **Evolu√ß√£o incremental**: Facilitar a expans√£o gradual do pipeline de ETL
- **Custo controlado**: Priorizar previsibilidade de custos e opera√ß√£o enxuta
- **Simplicidade operacional**: Minimizar overhead de infraestrutura e manuten√ß√£o

## üèóÔ∏è Arquitetura

### Componentes Principais

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   FONTES DE     ‚îÇ    ‚îÇ     STAGING      ‚îÇ    ‚îÇ   DATA          ‚îÇ
‚îÇ     DADOS       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   (Parquet)      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  WAREHOUSE      ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ (PostgreSQL)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚ñº                       ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CONECTORES    ‚îÇ    ‚îÇ   PARQUET LAKE   ‚îÇ    ‚îÇ   TABELAS DW    ‚îÇ
‚îÇ  - SQL (PG/MS)  ‚îÇ    ‚îÇ  - Versionamento ‚îÇ    ‚îÇ  - Schema auto  ‚îÇ
‚îÇ  - HTTP/REST    ‚îÇ    ‚îÇ  - Auditoria     ‚îÇ    ‚îÇ  - Auditoria    ‚îÇ
‚îÇ  - MongoDB      ‚îÇ    ‚îÇ  - Reprocesso    ‚îÇ    ‚îÇ  - Lineage      ‚îÇ
‚îÇ  - Kafka/AMQP   ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ  - FTP/SSH      ‚îÇ    ‚îÇ                  ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fluxo de Processamento

1. **Extra√ß√£o**: Conectores especializados coletam dados das fontes
2. **Staging**: Dados s√£o persistidos em formato Parquet para rastreabilidade
3. **Transforma√ß√£o**: Dados s√£o normalizados e estruturados
4. **Carga**: Dados s√£o inseridos no Data Warehouse PostgreSQL
5. **Auditoria**: Metadados de execu√ß√£o s√£o registrados para monitoramento

### Stack Tecnol√≥gica

- **Python 3.11+**: Linguagem principal
- **uv**: Gerenciamento de depend√™ncias e execu√ß√£o
- **Polars**: Manipula√ß√£o eficiente de DataFrames
- **SQLAlchemy 2.0+**: ORM e abstra√ß√£o de banco de dados
- **PostgreSQL**: Data Warehouse de destino
- **Parquet**: Formato de staging para dados
- **Pydantic**: Valida√ß√£o de configura√ß√µes
- **Mage.ai**: Orquestra√ß√£o de pipelines (opcional)

## ‚ú® Caracter√≠sticas Principais

### üîå Conectores Extens√≠veis

Sistema de conectores baseado em protocolo com descoberta autom√°tica de classes:

```python
# Factory pattern com autodescoberta
connector = create_connector({
    "protocol": "postgres",
    "host": "localhost",
    "database": "analytics"
})
```

### üì¶ Staging Inteligente

Camada intermedi√°ria usando Parquet para:
- Rastreabilidade completa de dados
- Capacidade de reprocessamento
- Desacoplamento entre ingest√£o e carga
- Versionamento autom√°tico por timestamp

### üîç Auditoria Completa

Sistema de auditoria integrado que registra:
- Execu√ß√µes de pipeline com UUID √∫nico
- Contagem de linhas processadas
- Dura√ß√£o de execu√ß√£o
- Status de sucesso/falha
- Metadados de fonte e destino

### üöÄ CLI Unificada

Interface de linha de comando para:
- Execu√ß√£o de pipelines ETL
- Testes de conectividade
- Monitoramento via logs estruturados

### üìä Schema Auto-adaptativo

Cria√ß√£o autom√°tica de esquemas no DW baseado na estrutura dos dados de entrada.

## üîå Conectores Dispon√≠veis

### Bancos de Dados SQL
- **PostgreSQL**: Conectividade nativa com psycopg
- **Microsoft SQL Server**: Suporte via pyodbc
- **Oracle**: Conectividade com oracledb
- **SQLite**: Para desenvolvimento e testes

### APIs e Web Services
- **HTTP/REST**: Cliente HTTP com autentica√ß√£o
- **SOAP**: Integra√ß√£o com servi√ßos SOAP via zeep

### NoSQL
- **MongoDB**: Conectividade nativa com serializa√ß√£o de ObjectId
- **Cassandra**: Suporte a clusters Cassandra
- **Neo4j**: Conectividade via driver Bolt

### Arquivos e Transfer√™ncia
- **FTP/SFTP**: Transfer√™ncia de arquivos via FTP e SSH
- **WebDAV**: Integra√ß√£o com servi√ßos WebDAV
- **SSH/SFTP**: Acesso seguro via paramiko

### Streaming e Messaging
- **Apache Kafka**: Consumo de mensagens via confluent-kafka
- **RabbitMQ (AMQP)**: Mensageria via pika
- **NATS**: Streaming de mensagens via nats-py

### SaaS e Cloud Services
- **Google Sheets**: Integra√ß√£o via gspread com Service Account

## üì¶ Instala√ß√£o

### Requisitos

- Python 3.11+
- `uv` (gerenciador de pacotes)

### Instala√ß√£o

No diret√≥rio do projeto:

```bash
uv sync --group dev
```

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de Ambiente

Crie um arquivo `.env` no diret√≥rio raiz:

```bash
# Data Warehouse (Destino)
DW_HOST=localhost
DW_PORT=5432
DW_DATABASE=dw_db
DW_USERNAME=dw_user
DW_PASSWORD=dw_password

# Exemplo: Fonte PostgreSQL
PG_SOURCE_HOST=source-db.example.com
PG_SOURCE_PORT=5432
PG_SOURCE_DATABASE=prod_db
PG_SOURCE_USERNAME=readonly_user
PG_SOURCE_PASSWORD=secure_password
```

### Arquivos de Configura√ß√£o

Os conectores podem ser configurados via JSON ou YAML:

#### PostgreSQL (postgres_connector.yaml)
```yaml
protocol: postgres
host: source-db.example.com
port: 5432
database: production
username: etl_user
password: secure_password
```

#### HTTP/REST (api_connector.yaml)
```yaml
protocol: http
base_url: https://api.example.com
token: your_api_token
headers:
  Content-Type: application/json
  X-Custom-Header: custom_value
```

#### MongoDB (mongo_connector.yaml)
```yaml
protocol: mongodb
host: mongo.example.com
port: 27017
database: analytics
username: mongo_user
password: mongo_password
auth_source: admin
```

Veja mais exemplos em [`etl/connections/examples/`](etl/connections/examples/).

## üöÄ Uso

### CLI Principal

#### Executar Pipeline ETL

```bash
cd etl && uv run --with-requirements requirements.txt python -m etl.cli run \
  --config ../config/postgres_source.yaml \
  --query "SELECT * FROM users WHERE updated_at > '2024-01-01'" \
  --source "postgres_prod" \
  --table "stg_users" \
  --lake ../lake \
  --schema "staging" \
  --pipeline "daily_user_sync"
```

#### Testar Conex√µes

```bash
# Testar conex√£o do Data Warehouse
cd etl && uv run --with-requirements requirements.txt python -m etl.cli test-connection --source dw

# Testar conex√£o de fonte
cd etl && uv run --with-requirements requirements.txt python -m etl.cli test-connection --config ../config/postgres_source.yaml
```

### Via Makefile (Recomendado)

```bash
# Executar pipeline
make run-pipeline CONFIG=config/postgres_source.yaml QUERY="SELECT * FROM users" SOURCE=postgres_prod TABLE=stg_users

# Testar conex√£o DW
make test-dw

# Executar todos os testes
make make-tests-all
```

### Programaticamente

```python
from etl.pipeline.runner import run_pipeline
from etl.connections.dw_destination import get_dw_engine

# Configura√ß√£o do conector
config = {
    "protocol": "postgres",
    "host": "source-db.com",
    "database": "production",
    "username": "etl_user",
    "password": "password"
}

# Engine do DW
dw_engine = get_dw_engine()

# Executar pipeline
result = run_pipeline(
    connector_config=config,
    query="SELECT * FROM orders WHERE date >= '2024-01-01'",
    source_name="prod_orders",
    target_table="stg_orders",
    lake_path="./lake",
    dw_engine=dw_engine,
    schema="staging"
)

print(f"Pipeline status: {result['status']}")
print(f"Rows loaded: {result['rows_loaded']}")
```

## üìö Exemplos Pr√°ticos

### 1. Sincroniza√ß√£o PostgreSQL para DW

```python
# examples/postgres_to_dw.py
from etl.pipeline.runner import run_pipeline
from etl.connections.dw_destination import get_dw_engine

connector_config = {
    "protocol": "postgres",
    "env_prefix": "PG_SOURCE"  # Usa PG_SOURCE_HOST, PG_SOURCE_USER, etc.
}

dw_engine = get_dw_engine()

result = run_pipeline(
    connector_config=connector_config,
    query="SELECT * FROM public.users",
    source_name="postgres_prod",
    target_table="stg_users",
    lake_path="./lake",
    dw_engine=dw_engine,
    pipeline_name="postgres_sync"
)
```

### 2. API REST para DW

```python
# examples/rest_api_to_dw.py
connector_config = {
    "protocol": "http",
    "base_url": "https://jsonplaceholder.typicode.com",
}

result = run_pipeline(
    connector_config=connector_config,
    query="/users",  # endpoint da API
    source_name="placeholder_api",
    target_table="stg_api_users",
    lake_path="./lake",
    dw_engine=dw_engine,
    pipeline_name="api_sync"
)
```

### 3. Pipeline Incremental

```python
# examples/incremental_postgres_to_dw.py
from etl.connections.sources.sql.incremental import fetch_incremental_rows

# Busca apenas registros novos baseado em watermark
rows, new_watermark = fetch_incremental_rows(
    engine=source_engine,
    table_name="orders",
    watermark_column="updated_at",
    last_watermark=datetime(2024, 1, 1),
    batch_size=1000
)
```

## üé≠ Integra√ß√£o com Mage.ai

O framework inclui integra√ß√£o completa com Mage.ai para orquestra√ß√£o visual de pipelines.

### Setup da Integra√ß√£o

```bash
# Subir infraestrutura (PostgreSQL + Mage)
make infra-up

# Testar integra√ß√£o
make integration-test
```

### Blocos Mage Customizados

#### Data Loader
```python
# mage_blocks/data_loaders/etl_source_extractor.py
@data_loader
def extract_data(*args, **kwargs):
    return execute_etl_extraction(
        config_path="/app/configs/postgres_source.json",
        query="SELECT * FROM users WHERE active = true",
        source_name="active_users"
    )
```

#### Data Exporter
```python
# mage_blocks/data_exporters/etl_dw_loader.py
@data_exporter
def load_to_dw(data, *args, **kwargs):
    return load_to_data_warehouse(
        data=data,
        target_table="stg_active_users",
        schema="staging"
    )
```

### Acesso √† Interface

Ap√≥s iniciar a infraestrutura:
- **Mage.ai UI**: http://localhost:6789
- **PostgreSQL**: localhost:5432

## üß™ Testes

### Estrutura de Testes

- **Testes unit√°rios**: Valida√ß√£o de componentes individuais
- **Testes de integra√ß√£o**: Valida√ß√£o de fluxos completos
- **Testes de conex√£o**: Valida√ß√£o de conectividade com fontes

### Executar Testes

```bash
# Todos os testes
make make-tests-all

# Testes espec√≠ficos
make make-tests-conn      # Testes de conex√£o
make make-tests-staging   # Testes de staging
make make-tests-pipeline  # Testes de pipeline

# Teste espec√≠fico
uv run --with pytest --with-requirements etl/requirements.txt pytest -q tests/test_connections.py -k test_postgres
```

### Testes de Integra√ß√£o com Mage

```bash
# Testar integra√ß√£o completa
make integration-test

# Apenas integra√ß√£o Mage
make test-mage-integration
```

## üèóÔ∏è Infraestrutura

### Docker Compose

O projeto inclui um `docker-compose.yml` completo com:

- **PostgreSQL 16**: Data Warehouse de destino
- **Mage.ai**: Orquestrador visual de pipelines
- **Volumes persistentes**: Para dados e configura√ß√µes
- **Networking**: Conectividade entre servi√ßos

### Comandos de Infraestrutura

```bash
# Iniciar servi√ßos
make infra-up

# Parar servi√ßos
make infra-down

# Status dos servi√ßos
make infra-status
```

### Terraform (Opcional)

Configura√ß√µes para deploy em cloud:

- **AWS**: `infra/aws.tf`
- **Azure**: `infra/azure.tf`
- **Vari√°veis**: `infra/terraform.tfvars.example`

## üìñ Documenta√ß√£o

### Gerar Documenta√ß√£o

```bash
# Servir documenta√ß√£o localmente
make docs-serve  # Acesse http://localhost:8000

# Gerar site est√°tico
make docs-build  # Sa√≠da em docs/site/

# Limpar documenta√ß√£o
make docs-clean
```

### Guias Dispon√≠veis

- **Instala√ß√£o**: `docs/setup/installation.md`
- **Arquitetura**: `docs/setup/architecture.md`
- **Novo Conector**: `docs/guides/new-connector.md`
- **Pipeline**: `docs/guides/pipeline.md`
- **Orquestra√ß√£o Mage**: `docs/guides/mage-orchestration.md`

## üìÅ Estrutura do Projeto

```
single_node_dw/
‚îú‚îÄ‚îÄ etl/                          # Framework ETL principal
‚îÇ   ‚îú‚îÄ‚îÄ connections/              # Sistema de conectores
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sources/              # Conectores de origem
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sql/              # Bancos SQL
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ http/             # APIs REST/HTTP
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nosql/            # Bancos NoSQL
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ streams/          # Streaming (Kafka, AMQP)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ftp/              # FTP/WebDAV
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ssh/              # SSH/SFTP
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ saas/             # SaaS (Google Sheets)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ examples/             # Exemplos de configura√ß√£o
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dw_destination.py     # Conex√£o DW destino
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/                 # Motor de execu√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ staging/                  # Camada de staging
‚îÇ   ‚îî‚îÄ‚îÄ cli.py                    # Interface CLI
‚îú‚îÄ‚îÄ examples/                     # Exemplos de uso
‚îú‚îÄ‚îÄ mage_blocks/                  # Blocos customizados Mage
‚îú‚îÄ‚îÄ mage_templates/               # Templates de pipeline
‚îú‚îÄ‚îÄ tests/                        # Suite de testes
‚îÇ   ‚îî‚îÄ‚îÄ integration/              # Testes de integra√ß√£o
‚îú‚îÄ‚îÄ docs/                         # Documenta√ß√£o
‚îú‚îÄ‚îÄ infra/                        # Infraestrutura como c√≥digo
‚îú‚îÄ‚îÄ lake/                         # Data Lake (Parquet)
‚îú‚îÄ‚îÄ docker-compose.yml            # Orquestra√ß√£o de servi√ßos
‚îú‚îÄ‚îÄ Makefile                      # Comandos de automa√ß√£o
‚îî‚îÄ‚îÄ pyproject.toml               # Configura√ß√£o do projeto
```

### Componentes Principais

#### Framework ETL (`etl/`)
- **`connections/`**: Sistema extens√≠vel de conectores
- **`pipeline/`**: Motor de execu√ß√£o de pipelines
- **`staging/`**: Camada de staging com Parquet
- **`cli.py`**: Interface de linha de comando

#### Conectores (`etl/connections/sources/`)
- **`factory.py`**: Factory pattern para instancia√ß√£o
- **`base_connector.py`**: Interface abstrata de conectores
- **`data_contract.py`**: Contratos de dados padronizados

#### Staging (`etl/staging/`)
- **`writer.py`**: Escrita de dados em Parquet
- **`loader.py`**: Carregamento de Parquet para DW
- **`audit.py`**: Sistema de auditoria
- **`dw_schema.py`**: Gerenciamento de esquemas DW

#### Mage Integration (`mage_blocks/`)
- **`custom/`**: Blocos personalizados
- **`data_loaders/`**: Extratores de dados
- **`data_exporters/`**: Carregadores de dados

## üîß Desenvolvimento

### Criando um Novo Conector

1. **Crie o m√≥dulo do conector**:
```python
# etl/connections/sources/myprotocol/connector.py
from ..base_connector import BaseConnector
from ..data_contract import IngestionResult

class MyProtocolConnector(BaseConnector):
    def connect(self):
        # Implementar l√≥gica de conex√£o
        pass
    
    def fetch_data(self, query: str) -> IngestionResult:
        # Implementar l√≥gica de extra√ß√£o
        pass
    
    def close(self):
        # Implementar limpeza de recursos
        pass
```

2. **Configure exemplo**:
```yaml
# etl/connections/examples/myprotocol_connector.example.yaml
protocol: myprotocol
endpoint: https://api.example.com
api_key: your_api_key
```

3. **Teste o conector**:
```python
# tests/test_myprotocol.py
def test_myprotocol_connector():
    connector = create_connector({
        "protocol": "myprotocol",
        "endpoint": "https://api.test.com"
    })
    assert isinstance(connector, MyProtocolConnector)
```

### Contribuindo

1. Fork o reposit√≥rio
2. Crie uma branch para sua feature (`git checkout -b feature/nova-funcionalidade`)
3. Implemente as mudan√ßas com testes
4. Execute a suite de testes (`make make-tests-all`)
5. Commit suas mudan√ßas (`git commit -am 'Adiciona nova funcionalidade'`)
6. Push para a branch (`git push origin feature/nova-funcionalidade`)
7. Abra um Pull Request

---

## üìù Licen√ßa

Este projeto est√° licenciado sob os termos da licen√ßa MIT.

## ü§ù Suporte

Para d√∫vidas, problemas ou sugest√µes:

1. Abra uma [issue](../../issues)
2. Consulte a [documenta√ß√£o](docs/)
3. Execute os testes de diagn√≥stico: `make integration-test`

---

**Feito com ‚ù§Ô∏è para simplificar ETL em ambientes single-node**