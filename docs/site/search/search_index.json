{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Framework de Data Warehouse Single-Node","text":"<p>Este projeto \u00e9 um framework de Data Warehouse Single-Node projetado para entregar baixo custo operacional e alta efici\u00eancia em cen\u00e1rios de dados pequenos e m\u00e9dios.</p> <p>A proposta \u00e9 simples: reduzir complexidade de infraestrutura sem abrir m\u00e3o de organiza\u00e7\u00e3o, rastreabilidade e qualidade no processo de ETL.</p>"},{"location":"#objetivo","title":"Objetivo","text":"<ul> <li>Consolidar dados de m\u00faltiplas fontes em um ambiente \u00fanico.</li> <li>Facilitar evolu\u00e7\u00e3o incremental do pipeline de ETL.</li> <li>Priorizar previsibilidade de custos e opera\u00e7\u00e3o enxuta.</li> </ul>"},{"location":"#stack-tecnica","title":"Stack T\u00e9cnica","text":"<ul> <li>Python como linguagem principal de desenvolvimento dos pipelines e conectores.</li> <li>uv para gerenciamento de pacotes e execu\u00e7\u00e3o consistente de scripts e testes.</li> </ul> <p>Essa combina\u00e7\u00e3o permite setup r\u00e1pido, depend\u00eancias controladas e manuten\u00e7\u00e3o simplificada.</p>"},{"location":"#arquitetura","title":"Arquitetura","text":"<p>A arquitetura \u00e9 dividida em tr\u00eas blocos claros:</p> <ol> <li>Conectores</li> <li>Respons\u00e1veis pela integra\u00e7\u00e3o com fontes de dados (SQL, APIs, arquivos, streams e servi\u00e7os externos).</li> <li> <p>Isolam detalhes de autentica\u00e7\u00e3o, leitura e normaliza\u00e7\u00e3o inicial.</p> </li> <li> <p>Staging em arquivos</p> </li> <li>Camada intermedi\u00e1ria para persist\u00eancia tempor\u00e1ria dos dados extra\u00eddos.</li> <li> <p>Melhora rastreabilidade, facilita reprocessamentos e desacopla ingest\u00e3o de carga anal\u00edtica.</p> </li> <li> <p>DW Relacional</p> </li> <li>Camada final de armazenamento anal\u00edtico estruturado.</li> <li> <p>Organiza dados para consumo por BI, relat\u00f3rios operacionais e an\u00e1lises de neg\u00f3cio.</p> </li> <li> <p>Pipeline Orchestrator &amp; CLI</p> </li> <li>Motor de execu\u00e7\u00e3o que coordena o fluxo fim-a-fim (Extra\u00e7\u00e3o -&gt; Staging -&gt; Carga).</li> <li>CLI unificada para execu\u00e7\u00e3o de pipelines, testes de conex\u00e3o e monitoramento via logs de auditoria.</li> </ol>"},{"location":"#resultado-esperado","title":"Resultado esperado","text":"<p>Com essa abordagem, o framework oferece uma base pragm\u00e1tica para ETL em ambiente single-node: implementa\u00e7\u00e3o r\u00e1pida, opera\u00e7\u00e3o est\u00e1vel e custo controlado.</p>"},{"location":"next-steps/","title":"Single Node DW - Status Report","text":"<p>O plano inicial de implementa\u00e7\u00e3o foi conclu\u00eddo com sucesso. Todos os componentes do ecossistema de Data Warehouse Single-Node est\u00e3o operacionais e testados.</p>"},{"location":"next-steps/#estado-atual-2026-02-21","title":"Estado Atual (2026-02-21)","text":"<ul> <li>Extra\u00e7\u00e3o: 15+ conectores (SQL, NoSQL, API, Arquivos, Streams). \u2705</li> <li>Staging: Convers\u00e3o <code>IngestionResult</code> -&gt; Parquet com particionamento. \u2705</li> <li>Schema Management: Cria\u00e7\u00e3o autom\u00e1tica de tabelas e evolu\u00e7\u00e3o de schema no DW. \u2705</li> <li>Carga (Loader): Carregamento de Parquet para PostgreSQL com rastreabilidade de arquivo. \u2705</li> <li>Auditoria: Log centralizado de execu\u00e7\u00f5es (<code>etl_audit_log</code>). \u2705</li> <li>Orquestra\u00e7\u00e3o: Pipeline Runner coordenando o fluxo fim-a-fim. \u2705</li> <li>Interface: CLI unificada para opera\u00e7\u00e3o e testes. \u2705</li> <li>Documenta\u00e7\u00e3o: Guias de uso, arquitetura e instala\u00e7\u00e3o atualizados. \u2705</li> <li>Testes: 70 testes automatizados cobrindo 100% dos componentes cr\u00edticos. \u2705</li> </ul>"},{"location":"next-steps/#estrutura-final-do-projeto","title":"Estrutura Final do Projeto","text":"<pre><code>etl/\n\u251c\u2500\u2500 cli.py                       # CLI Entrypoint\n\u251c\u2500\u2500 connections/                 # Conectores de Origem\n\u251c\u2500\u2500 staging/                     # Writer, Loader, Audit, Schema\n\u2514\u2500\u2500 pipeline/                    # Pipeline Runner (Orquestrador)\n\nexamples/                        # Exemplos Pr\u00e1ticos (Postgres, REST, Incremental)\ntests/                           # Su\u00edte de Testes (Connections, Staging, Pipeline, CLI)\ndocs/                            # Documenta\u00e7\u00e3o (MkDocs)\ninfra/                           # Terraform e Docker\n</code></pre>"},{"location":"next-steps/#proximas-evolucoes-sugeridas","title":"Pr\u00f3ximas Evolu\u00e7\u00f5es Sugeridas","text":"<ol> <li>Transforma\u00e7\u00f5es dbt: Integrar dbt-core para transforma\u00e7\u00f5es SQL dentro do DW.</li> <li>Dashboard de Auditoria: Criar views ou dashboards simples para visualizar falhas de pipeline.</li> <li>Alertas: Adicionar hooks para envio de notifica\u00e7\u00f5es (Slack/Email) em caso de erro.</li> <li>Data Quality: Integrar testes de schema e valores (Expectations) durante a carga.</li> </ol>"},{"location":"guides/new-connector/","title":"Guia: Criar um Novo Conector","text":"<p>Este guia usa o padr\u00e3o do <code>HTTPConnector</code> como refer\u00eancia de estilo e estrutura para novos conectores no pacote <code>etl/connections/sources</code>.</p>"},{"location":"guides/new-connector/#objetivo-do-padrao","title":"Objetivo do padr\u00e3o","text":"<p>Todo conector deve:</p> <ul> <li>Implementar a interface <code>BaseConnector</code> (<code>connect</code>, <code>fetch_data</code>, <code>close</code>).</li> <li>Retornar dados no contrato <code>IngestionResult</code> com itens <code>IngestedItem</code>.</li> <li>Usar configura\u00e7\u00e3o em camadas via <code>load_connection_config</code>.</li> <li>Validar configura\u00e7\u00e3o com Pydantic (<code>&lt;Protocol&gt;Config.model_validate(...)</code>).</li> <li>Registrar logs com <code>get_logger(...)</code> e mascaramento com <code>redact_config(...)</code>.</li> <li>Expor erros claros para facilitar debugging.</li> </ul>"},{"location":"guides/new-connector/#estrutura-recomendada","title":"Estrutura recomendada","text":"<p>Crie um diret\u00f3rio de protocolo em <code>etl/connections/sources/&lt;protocol&gt;/</code> com:</p> <ul> <li><code>config.py</code>: modelo Pydantic de configura\u00e7\u00e3o.</li> <li><code>connector.py</code>: implementa\u00e7\u00e3o do conector.</li> <li><code>__init__.py</code>: exporta\u00e7\u00f5es p\u00fablicas (quando aplic\u00e1vel).</li> </ul>"},{"location":"guides/new-connector/#passo-1-definir-o-modelo-de-configuracao","title":"Passo 1: Definir o modelo de configura\u00e7\u00e3o","text":"<p>Exemplo (<code>config.py</code>):</p> <pre><code>from pydantic import BaseModel, ConfigDict, Field\n\n\nclass NewProtocolConfig(BaseModel):\n    model_config = ConfigDict(extra=\"ignore\")\n\n    endpoint: str\n    api_key: str | None = None\n    timeout_seconds: int = Field(default=30, ge=1)\n</code></pre>"},{"location":"guides/new-connector/#passo-2-implementar-o-conector","title":"Passo 2: Implementar o conector","text":"<p>Exemplo base (<code>connector.py</code>) no mesmo estilo do <code>HTTPConnector</code>:</p> <pre><code>from ..._config import load_connection_config\nfrom ..._logging import get_logger, redact_config\nfrom ..base_connector import BaseConnector\nfrom ..data_contract import IngestedItem, IngestionResult\nfrom .config import NewProtocolConfig\n\n\nclass NewProtocolConnector(BaseConnector):\n    def __init__(\n        self,\n        endpoint: str,\n        api_key: str | None = None,\n        timeout_seconds: int = 30,\n        config: dict | None = None,\n        file_path: str | None = None,\n        env_prefix: str = \"NEWPROTOCOL\",\n    ):\n        merged_config = load_connection_config(\n            config,\n            file_path=file_path,\n            env_prefix=env_prefix,\n            required=(\"endpoint\",),\n            defaults={\n                \"timeout_seconds\": timeout_seconds,\n            },\n            overrides={\n                \"endpoint\": endpoint,\n                \"api_key\": api_key,\n                \"timeout_seconds\": timeout_seconds,\n            },\n        )\n        self.config = NewProtocolConfig.model_validate(merged_config)\n        self.logger = get_logger(\"sources.newprotocol.connector\")\n        self._client = None\n\n    def connect(self) -&gt; None:\n        safe_config = redact_config(self.config.model_dump())\n        self.logger.info(\"Connecting NewProtocol connector with config=%s\", safe_config)\n\n        self._client = object()\n        self.logger.info(\"NewProtocol connector ready\")\n\n    def fetch_data(self, query: str) -&gt; IngestionResult:\n        if not query.strip():\n            raise ValueError(\"query cannot be empty.\")\n\n        if self._client is None:\n            raise RuntimeError(\"NewProtocol connector is not connected. Call connect() first.\")\n\n        payload = {\"endpoint\": self.config.endpoint, \"query\": query, \"status\": \"ok\"}\n\n        return IngestionResult(\n            protocol=\"newprotocol\",\n            success=True,\n            items=[IngestedItem(payload=payload)],\n            metadata={\"client\": \"default\"},\n        )\n\n    def close(self) -&gt; None:\n        self.logger.info(\"Closing NewProtocol connector\")\n        self._client = None\n</code></pre>"},{"location":"guides/new-connector/#passo-3-convencoes-obrigatorias","title":"Passo 3: Conven\u00e7\u00f5es obrigat\u00f3rias","text":"<ul> <li>Use argumentos expl\u00edcitos e nomeados no <code>__init__</code> (evite <code>*args</code> e <code>**kwargs</code>).</li> <li>Defina <code>env_prefix</code> consistente com o protocolo (ex.: <code>REST</code>, <code>PG</code>, <code>MONGODB</code>).</li> <li>Garanta mensagens de erro orientadas \u00e0 a\u00e7\u00e3o (<code>Call connect() first</code>, campo obrigat\u00f3rio ausente, etc.).</li> <li>Mantenha o ciclo de vida completo: abrir recursos em <code>connect()</code> e liberar em <code>close()</code>.</li> </ul>"},{"location":"guides/new-connector/#passo-4-exemplo-de-variaveis-de-ambiente","title":"Passo 4: Exemplo de vari\u00e1veis de ambiente","text":"<p>Para <code>env_prefix=\"NEWPROTOCOL\"</code>, o loader mapeia campos como:</p> <pre><code>NEWPROTOCOL_ENDPOINT=https://api.example.com\nNEWPROTOCOL_API_KEY=token-value\nNEWPROTOCOL_TIMEOUT_SECONDS=30\n</code></pre>"},{"location":"guides/new-connector/#passo-5-usar-via-factory-dinamica","title":"Passo 5: Usar via factory din\u00e2mica","text":"<p>Com <code>protocol</code> configurado, o conector pode ser criado por <code>create_connector</code>:</p> <pre><code>from connections.sources.factory import create_connector\n\nconnector = create_connector(\n    {\n        \"protocol\": \"newprotocol\",\n        \"endpoint\": \"https://api.example.com\",\n        \"api_key\": \"token-value\",\n    }\n)\n\nconnector.connect()\nresult = connector.fetch_data(\"/health\")\nconnector.close()\n</code></pre>"},{"location":"guides/new-connector/#passo-6-validar-com-testes","title":"Passo 6: Validar com testes","text":"<p>Rode os testes do projeto com <code>uv</code> + <code>pytest</code>:</p> <pre><code>cd single_node_dw\nuv run --with pytest --with-requirements etl/requirements.txt pytest -q tests/test_connections.py\n</code></pre> <p>Para focar em um caso espec\u00edfico:</p> <pre><code>uv run --with pytest --with-requirements etl/requirements.txt pytest -q tests/test_connections.py -k newprotocol\n</code></pre>"},{"location":"guides/new-connector/#checklist-de-revisao","title":"Checklist de revis\u00e3o","text":"<ul> <li>Contrato <code>BaseConnector</code> implementado.</li> <li>Configura\u00e7\u00e3o validada com Pydantic.</li> <li>Logs com redaction de dados sens\u00edveis.</li> <li><code>IngestionResult</code> preenchido com <code>protocol</code>, <code>success</code>, <code>items</code> e <code>metadata</code>.</li> <li><code>connect()</code> e <code>close()</code> sem vazamento de recurso.</li> <li>Erros expl\u00edcitos e f\u00e1ceis de diagnosticar.</li> </ul>"},{"location":"guides/pipeline/","title":"ETL Pipeline Runner","text":"<p>The pipeline runner is the core orchestration layer of the Single Node DW. It manages the full lifecycle of data: extraction, staging as Parquet, and loading into the PostgreSQL Data Warehouse with full auditing.</p>"},{"location":"guides/pipeline/#infrastructure-setup","title":"Infrastructure Setup","text":"<p>Before running pipelines, ensure your PostgreSQL Data Warehouse is running. You can use the provided Docker Compose configuration:</p> <pre><code>make infra-up\n</code></pre> <p>This starts a PostgreSQL instance accessible at <code>localhost:5432</code>.</p>"},{"location":"guides/pipeline/#configuration","title":"Configuration","text":"<p>Configure your connections in a <code>.env</code> file (see <code>.env.example</code> for reference). At a minimum, you need the Data Warehouse credentials:</p> <pre><code>DW_HOST=localhost\nDW_PORT=5432\nDW_DATABASE=dw\nDW_USERNAME=postgres\nDW_PASSWORD=postgres\n</code></pre>"},{"location":"guides/pipeline/#running-pipelines-via-cli","title":"Running Pipelines via CLI","text":"<p>You can run pipelines using the <code>etl.cli</code> module. The CLI expects a connector configuration file (JSON or YAML).</p>"},{"location":"guides/pipeline/#example-syncing-from-a-rest-api","title":"Example: Syncing from a REST API","text":"<p>Create a <code>connector.json</code>:</p> <pre><code>{\n  \"protocol\": \"http\",\n  \"base_url\": \"https://jsonplaceholder.typicode.com\"\n}\n</code></pre> <p>Run the pipeline:</p> <pre><code>python -m etl.cli run \n  --config connector.json \n  --query \"/users\" \n  --source api_users \n  --table stg_users \n  --lake ./lake\n</code></pre>"},{"location":"guides/pipeline/#parameters","title":"Parameters","text":"<ul> <li><code>--config</code>: Path to the source connector configuration.</li> <li><code>--query</code>: The resource to fetch (SQL query for databases, relative path for HTTP).</li> <li><code>--source</code>: A logical name for the source system.</li> <li><code>--table</code>: The target table name in the DW.</li> <li><code>--lake</code>: Local directory where Parquet files will be staged.</li> <li><code>--schema</code>: (Optional) Target schema in the DW (default: <code>public</code>).</li> <li><code>--pipeline</code>: (Optional) Logical name for this pipeline run (default: <code>default</code>).</li> </ul>"},{"location":"guides/pipeline/#testing-connections","title":"Testing Connections","text":"<p>Verify your setup with the <code>test-connection</code> command:</p> <pre><code># Test DW connection\npython -m etl.cli test-connection --source dw\n\n# Test a source connection\npython -m etl.cli test-connection --config connector.json\n</code></pre>"},{"location":"guides/pipeline/#audit-log","title":"Audit Log","text":"<p>Every pipeline run is recorded in the <code>etl_audit_log</code> table. You can query it to check for status and performance:</p> <pre><code>SELECT \n    pipeline_name, \n    source_name, \n    status, \n    rows_loaded, \n    started_at, \n    finished_at - started_at as duration\nFROM etl_audit_log\nORDER BY started_at DESC;\n</code></pre>"},{"location":"guides/pipeline/#error-handling","title":"Error Handling","text":"<p>If a pipeline fails: 1. The error is logged to the console (stderr). 2. A record is written to <code>etl_audit_log</code> with <code>status = 'failure'</code> and the <code>error_message</code>. 3. The process exits with code <code>1</code>.</p>"},{"location":"setup/architecture/","title":"Arquitetura Single-Node","text":"<p>A arquitetura deste projeto adota um modelo Single-Node: ingest\u00e3o, staging, transforma\u00e7\u00e3o e carga anal\u00edtica operam no mesmo ambiente Linux, com separa\u00e7\u00e3o l\u00f3gica por camadas.</p>"},{"location":"setup/architecture/#visao-do-fluxo","title":"Vis\u00e3o do fluxo","text":"<pre><code>flowchart LR\n    A[Source Systems] --&gt; B[Connectors Package]\n    B --&gt; C[Local Staging Area (Files)]\n    C --&gt; D[ETL Processing]\n    D --&gt; E[PostgreSQL DW]\n</code></pre>"},{"location":"setup/architecture/#componentes","title":"Componentes","text":"<ul> <li>Source Systems: bancos relacionais, APIs, arquivos e servi\u00e7os externos.</li> <li>Connectors Package: camada de integra\u00e7\u00e3o que padroniza autentica\u00e7\u00e3o, leitura e contrato de dados.</li> <li>Local Staging Area (Files): persist\u00eancia intermedi\u00e1ria em arquivos para desacoplar extra\u00e7\u00e3o e transforma\u00e7\u00e3o.</li> <li>ETL Processing: normaliza\u00e7\u00e3o, valida\u00e7\u00f5es e aplica\u00e7\u00e3o de regras de neg\u00f3cio.</li> <li>PostgreSQL DW: armazenamento anal\u00edtico relacional para BI, relat\u00f3rios e consultas.</li> </ul>"},{"location":"setup/architecture/#por-que-uma-unica-vm-linux","title":"Por que uma \u00fanica VM Linux","text":"<p>Escolhemos executar toda a solu\u00e7\u00e3o em uma \u00fanica VM Linux para reduzir custo e complexidade operacional:</p> <ul> <li>Menor custo de infraestrutura: evita gastos com clusteriza\u00e7\u00e3o e m\u00faltiplos servi\u00e7os distribu\u00eddos.</li> <li>Opera\u00e7\u00e3o simplificada: provisioning, monitoramento, backup e troubleshooting em um \u00fanico ponto.</li> <li>Menor lat\u00eancia interna: troca de dados entre camadas no mesmo host, sem depend\u00eancia de rede entre n\u00f3s.</li> <li>Onboarding r\u00e1pido: ambiente previs\u00edvel para desenvolvimento e suporte.</li> <li>Escala adequada ao contexto: atende cargas pequenas e m\u00e9dias com boa efici\u00eancia.</li> </ul> <p>Esse desenho prioriza pragmatismo: arquitetura clara, custo controlado e manuten\u00e7\u00e3o enxuta.</p>"},{"location":"setup/installation/","title":"Instala\u00e7\u00e3o e Onboarding","text":"<p>Este guia cobre o onboarding de um novo desenvolvedor no framework de Data Warehouse Single-Node.</p>"},{"location":"setup/installation/#1-pre-requisitos","title":"1) Pr\u00e9-requisitos","text":"<ul> <li>Python 3.11+</li> <li>Git</li> <li>Acesso ao reposit\u00f3rio</li> </ul>"},{"location":"setup/installation/#2-instalar-o-uv","title":"2) Instalar o <code>uv</code>","text":"<p>No Linux/macOS:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>No Windows (PowerShell):</p> <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <p>Validar instala\u00e7\u00e3o:</p> <pre><code>uv --version\n</code></pre>"},{"location":"setup/installation/#3-clonar-o-projeto-e-entrar-na-pasta","title":"3) Clonar o projeto e entrar na pasta","text":"<pre><code>git clone &lt;url-do-repositorio&gt;\ncd single_node_dw\n</code></pre>"},{"location":"setup/installation/#4-sincronizar-ambiente-com-uv-sync","title":"4) Sincronizar ambiente com <code>uv sync</code>","text":"<p>Com o <code>pyproject.toml</code> j\u00e1 configurado no projeto:</p> <pre><code>uv sync\n</code></pre>"},{"location":"setup/installation/#5-configurar-variaveis-de-ambiente-env","title":"5) Configurar vari\u00e1veis de ambiente (<code>.env</code>)","text":"<p>Crie seu <code>.env</code> local a partir do template:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Edite os valores conforme seu ambiente. Prefixos importantes:</p> <ul> <li><code>REST_</code>: integra\u00e7\u00f5es HTTP/REST (<code>REST_BASE_URL</code>, <code>REST_TOKEN</code>)</li> <li><code>PG_</code>: fonte PostgreSQL (<code>PG_HOST</code>, <code>PG_PORT</code>, <code>PG_DATABASE</code>, ...)</li> <li><code>DW_</code>: destino do Data Warehouse (<code>DW_HOST</code>, <code>DW_DATABASE</code>, ...)</li> <li><code>MSSQL_</code>, <code>ORACLE_</code>, <code>SQLITE_</code>: conectores SQL adicionais</li> <li><code>FTP_</code>, <code>WEBDAV_</code>, <code>SSH_</code>: conectores de arquivos/transfer\u00eancia</li> <li><code>MONGODB_</code>, <code>CASSANDRA_</code>, <code>NEO4J_</code>: conectores NoSQL</li> <li><code>KAFKA_</code>, <code>AMQP_</code>, <code>NATS_</code>: conectores de mensageria/stream</li> <li><code>GSHEETS_</code>, <code>SOAP_</code>: conectores SaaS/servi\u00e7os</li> </ul>"},{"location":"setup/installation/#6-validar-instalacao-com-testes-pytest","title":"6) Validar instala\u00e7\u00e3o com testes (<code>pytest</code>)","text":"<p>No diret\u00f3rio <code>single_node_dw/</code>, voc\u00ea pode rodar todos os testes ou su\u00edtes espec\u00edficas:</p> <pre><code># Todos os testes (Conectores, Staging, Pipeline, CLI)\nmake make-tests-all\n\n# Apenas conectores\nmake make-tests-conn\n\n# Apenas staging e carga (loader/audit)\nmake make-tests-staging\n\n# Apenas orquestrador de pipeline\nmake make-tests-pipeline\n</code></pre>"},{"location":"setup/installation/#7-primeiro-pipeline-quick-start","title":"7) Primeiro Pipeline (Quick Start)","text":"<p>Para testar o fluxo completo usando o CLI:</p> <pre><code># 1. Subir infraestrutura (Docker)\nmake infra-up\n\n# 2. Testar conex\u00e3o com o DW\nmake test-dw\n\n# 3. Executar um pipeline de exemplo (HTTP -&gt; DW)\nuv run --with-requirements etl/requirements.txt python -m etl.cli run \\\n  --config etl/connections/examples/http_connector.example.json \\\n  --query \"/users\" \\\n  --source api_test \\\n  --table stg_users_test \\\n  --lake ./lake\n</code></pre>"},{"location":"setup/installation/#8-troubleshooting-rapido","title":"8) Troubleshooting r\u00e1pido","text":"<p>Se ocorrer <code>ModuleNotFoundError</code> na coleta dos testes, repita os comandos com <code>--with-requirements etl/requirements.txt</code> (conforme acima).</p>"}]}